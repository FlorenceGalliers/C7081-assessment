fwd_variables <- which(variables_fwd == TRUE)
forward_train <- dummy_train[ ,fwd_variables]
forward_test <- dummy_test[, fwd_variables]
# Run linear model using 15 variables from forward selection
forward_model <- lm(price ~ .,
data = forward_train)
summary(forward_model)
# make predictions using this model on test data set
forward_pred <- predict(forward_model,
forward_test)
# calculate MSE of best 15 variable model from subset selection
forward_MSE <- mean((forward_test[, "price"] - forward_pred)^2)
# calculate RMSE of best 15 variable model from subset selection
forward_RMSE <- sqrt(forward_MSE)
# 5.0 Backward Stepwise Selection ####
backward_model <- regsubsets(price ~ .,
data = data,
nvmax = 25,
method = "backward")
summary(backward_model)
# Create loop for finding validation errors for a model of each size in backward selection
bwd_errors <- rep(NA, 25)
for(i in 1:25) {
coefi <- coef(backward_model, id = i)
pred <- test_matrix[ ,names(coefi)]%*%coefi
bwd_errors[i] = mean((test$price - pred)^2)
}
bwd_errors
which.min(bwd_errors) # minimum is 22
plot(bwd_errors, type = "b",
main = "Backward Selection Validation Errors",
xlab = "Model Size (number of variables included)",
ylab = "Validation Error",
cex.axis = 0.8)
# Create new test and train data sets containing only the best 2 variables as
# selected by forward selection
backward_summary <- summary(backward_model)
which_backward <- backward_summary$which
variables_bwd <- which_backward[22,]
bwd_variables <- which(variables_bwd == TRUE)
backward_train <- dummy_train[ ,bwd_variables]
backward_test <- dummy_test[, bwd_variables]
# Run linear model using 15 variables from backward selection
backward_model <- lm(price ~ .,
data = backward_train)
summary(backward_model)
# make predictions using this model on test data set
backward_pred <- predict(backward_model,
backward_test)
# calculate MSE of best 15 variable model from subset selection
backward_MSE <- mean((backward_test[, "price"] - backward_pred)^2)
# calculate RMSE of best 15 variable model from subset selection
backward_RMSE <- sqrt(backward_MSE)
# 6.0 Ridge Regression ####
# Fit a ridge regression, choose lambda by cross validation
set.seed(1)
# Split train and test data into x and y
x_train <- model.matrix(price ~ ., data = train)[,-1]
x_test <- model.matrix(price ~ ., data = test)[,-1]
y_train <- train$price
y_test <- test$price
# Finding optimal value of lambda using cross validation
cv.ridge <- cv.glmnet(x_train,
y_train,
alpha = 0)
# Assign it to best.lambda variable
best.lambda <- cv.ridge$lambda.min
best.lambda
plot(cv.ridge)
# Report test error of ridge regression
ridge.mod <- glmnet(x_train, y_train, alpha = 0)
ridge.pred <- predict(ridge.mod,
s = best.lambda,
newx = x_test)
ridge_MSE <- mean((ridge.pred - y_test)^2)
ridge_RMSE <- sqrt(ridge_MSE)
# Plot the various values of L1 norm, coefficient values and the number of variables.
plot(ridge.mod)
# 7.0 LASSO Regression ####
set.seed(1)
# Fit lasso model
lasso_mod <- glmnet(x_train,
y_train,
alpha = 1)
# Shows for a sequence of lambda, the values of the coefficients
# Numbers at the top of the graph show how many variables are included at each
# value of lambda - this is LASSO model selection capacity.
plot(lasso_mod,
xvar='lambda')
# Now we need to find which value of lambda minimises the difference between
# predicted and actual values, and therefore has lowest MSE
set.seed(1)
lasso_cv <- cv.glmnet(x_train,
y_train,
alpha = 1)
# Plot values of MSE against lambda, can see it is fairly stable but starts to
# increase slightly after less than 18 variables are included in the model
plot(lasso_cv)
# Assign the lowest value of lambda to an object
best_lamb <- lasso_cv$lambda.min
# Use this value of lambda to make predictions on test data set
lasso_pred <- predict(lasso_mod, s = best_lamb,
newx = x_test)
# Report test error and number of non zero coefficients for LASSO
lasso_MSE <- mean((lasso_pred - y_test)^2)
lasso_RMSE <- sqrt(lasso_MSE)
out <- glmnet(x_train, y_train, alpha = 1)
lasso_coef <- predict(out, type = "coefficients",
s = best_lamb)
lasso_coef # show coefficients for the model
lasso_mod$lambda # show values of lamba tried
lasso_mod$df[55] # Get number of variables in model with best lambda value
# A 34 variable model
estimates <- as.vector(coef(lasso_mod, s = best_lamb, exact = TRUE))
norm. <- sum(abs(estimates))
# Plot the different values of L1 norm and coefficient values and size of model
plot(lasso_mod, xlim = range(0, norm., as.vector(lasso_mod$beta)))
# Put red line in at the point where lambda was selected
abline(v = norm., col = "red")
# Plot actual vs predicted prices for this LASSO model.
plot(dummy_test[,"price"], lasso_pred,
main = "Actual vs Predicted House Prices
from the LASSO Model",
xlab = "Test Set Prices (actual), US Dollars",
ylab = "Predicted Prices, US Dollars")
abline(a = 0, b = 1, col = "blue")
# However from cross validation plot we can see that the error is quite stable
# as model decreases in size from 34 to 15
lasso_cv$cvm
# Lets assign a different value of lambda to an object
alt_lamb <- lasso_cv$lambda[33]
# Use this value of lambda to make predictions on test data set
lasso_pred2 <- predict(lasso_mod, s = alt_lamb,
newx = x_test)
# Report test error and number of non zero coefficients for LASSO
lasso2_MSE <- mean((lasso_pred2 - y_test)^2)
lasso2_RMSE <- sqrt(lasso2_MSE)
lasso2_coef <- predict(out, type = "coefficients",
s = alt_lamb)
lasso2_coef # show coefficients for the model
lasso_mod$df[33] # Get number of variables in model with alternative lambda
# value = 15 variables
#Now lets plot this value of lamba on the model
estimates <- as.vector(coef(lasso_mod, s = alt_lamb, exact = TRUE))
norm. <- sum(abs(estimates))
plot(lasso_mod, xlim = range(0, norm., as.vector(lasso_mod$beta)))
abline(v = norm., col = "red")
# This shows a much simpler model, by increasing lambda, MSE has also not increased by much
# 14.0 Trialling Linear Regression with houses only up to $1,000,000. ####
data2 <- data[data$price <= 1000, ]
set.seed(22) # set seed
n2 <- nrow(data2) # create variable with number of rows
train_index2 <- sample(1:n2, size = round(0.8*n), replace=FALSE)
train2 <- data2[train_index2 ,] # takes 80% of the data for training set
test2 <- data2[-train_index2 ,] # remaining 20% for the test set
x_train2 <- model.matrix(price ~ ., data = train2)[,-1]
x_test2 <- model.matrix(price ~ ., data = test2)[,-1]
y_train2 <- train2$price
y_test2 <- test2$price
set.seed(1)
# Fit lasso model
lasso_mod2 <- glmnet(x_train2,
y_train2,
alpha = 1)
# Shows for a sequence of lambda, the values of the coefficients
# Numbers at the top of the graph show how many variables are included at each
# value of lambda - this is LASSO model selection capacity.
plot(lasso_mod2,
xvar='lambda')
# Now we need to find which value of lambda minimises the difference between
# predicted and actual values, and therefore has lowest MSE
set.seed(1)
lasso_cv2 <- cv.glmnet(x_train2,
y_train2,
alpha = 1)
# Plot values of MSE against lambda, can see it is fairly stable but starts to
# increase slightly after less than 18 variables are included in the model
plot(lasso_cv2)
# Assign the lowest value of lambda to an object
best_lamb2 <- lasso_cv2$lambda.min
# Use this value of lambda to make predictions on test data set
lasso_pred2 <- predict(lasso_mod2, s = best_lamb2,
newx = x_test2)
# Report test error and number of non zero coefficients for LASSO
lasso_MSE2 <- mean((lasso_pred2 - y_test2)^2)
lasso_RMSE2 <- sqrt(lasso_MSE2)
out2 <- glmnet(x_train2, y_train2, alpha = 1)
lasso_coef2 <- predict(out2, type = "coefficients",
s = best_lamb2)
lasso_coef2 # show coefficients for the model
lasso_mod2$lambda # show values of lamba tried
lasso_mod$df[82] # Get number of variables in model with best lambda value
# A 40 variable model
# 8.0 Simple Decision Tree ####
set.seed(1)
# Fit a simple decision tree using data with only variables selected by best-subset
# selection as tree() function has a limit on number of variables
tree_house <- tree(price ~ .,
data = best_train)
summary(tree_house)
# Only one variable (sqft_living) is used to construct a tree with 5 nodes
plot(tree_house)
text(tree_house,
pretty = 0,
cex = 0.8)
# Tree indicates that sqft_living is the most important variable
cv_tree <- cv.tree(tree_house)
plot(cv_tree$size, cv_tree$dev, type = "b")
# Prune the tree to see if any improvements can be made
prunedtree <- prune.tree(tree_house, best = 5)
plot(prunedtree)
text(prunedtree,
pretty = 0,
cex = 0.8)
# The pruned tree and the unpruned tree have the same
# cross validation error so we can use either to make predictions
yhat <- predict(prunedtree,
newdata = best_test)
testdata <- best_test[ ,"price"]
plot(yhat, testdata)
abline(0, 1)
# Calculate test error of the tree
tree_MSE <- mean((yhat - testdata)^2)
tree_RMSE <- sqrt(tree_MSE)
# 9.0 Bagging ####
set.seed(1)
# Create a tree using bagging
rf_tree <- randomForest(price ~ .,
data = best_train,
mtry = 15, # m = p
importance = TRUE)
rf_tree
# Create predictions using this tree and test data
rf_pred <- predict(rf_tree,
newdata = best_test)
# Plot these predictions
plot(rf_pred, testdata)
abline(0,1)
# Calculate MSE and RMSE of these predictions
rf_MSE <- mean((rf_pred - testdata)^2)
rf_RMSE <- sqrt(rf_MSE)
# How many trees has it used by default?
rf_tree$ntree # this used 500 trees
# Plotting the error over number of trees shows that after around 100 trees
# the error is pretty constant
plot(rf_tree)
# So lets try bagging but with 100 trees
rf_100 <- randomForest(price ~ .,
data = best_train,
mtry = 15,
ntree = 100,
importance = TRUE)
# Predictions
rf_100_pred <- predict(rf_100,
newdata = best_test)
# Plot predictions
plot(rf_100_pred, testdata)
abline(0,1)
# Plot error against number of trees
plot(rf_100)
# Calculate MSE and RMSE
rf_100_MSE <- mean((rf_100_pred - testdata)^2)
rf_100_RMSE <- sqrt(rf_100_MSE)
# 10.0 RandomForests ####
# Now going to do RandomForest with m = 3
rf_m3 <- randomForest(price ~ .,
data = best_train,
mtry = 3,
ntree = 100,
importance = TRUE)
# Predictions
yhat_rf <- predict(rf_m3,
newdata = best_test)
# Calculate MSE and RMSE
rf_m3_MSE <- mean((yhat_rf-testdata)^2)
rf_m3_RMSE <- sqrt(rf_m3_MSE)
# Having m = 3 massively reduces the MSE
# Plot error over number of trees
plot(rf_m3)
# Can see that the error is pretty stable after around 20 trees - lets reduce
# it to 30 trees.
rf_m3_30 <- randomForest(price ~ .,
data = best_train,
mtry = 3,
ntree = 30,
importance = TRUE)
# Predictions
yhat_rf <- predict(rf_m3_30,
newdata = best_test)
# Calculate MSE and RMSE
rf_m3_30_MSE <- mean((yhat_rf-testdata)^2)
rf_m3_30_RMSE <- sqrt(rf_m3_30_MSE)
# This gives a slight reduction in MSE
# Take this forward as the final model for randomForests - rf_m3_30
# Lets look at importance of variance and plot this
importance(rf_m3_30)
varImpPlot(rf_m3_30)
# sqft_living is by far the most influential variable shown here.
# 11.0 Boosting ####
set.seed(1)
boost_house <- gbm(price ~ .,
data = best_train,
distribution = "gaussian",
n.trees = 5000,
interaction.depth = 4)
boost_house$shrinkage #lambda for boost_house is 0.1
summary(boost_house)
# From this summary it is clear the variable that accounts for the highest amount
# of variance is sqft_living, followed by no of bathrooms.
# The rest of the variables account for a tiny amount of variance in comparison.
# Lets produce some partial dependence plots for sqft_living and no of bathrooms
# these illustrate the marginal effect of the selected variables on the response
# after integrating out the other variables
plot(boost_house, i = "sqft_living")
plot(boost_house, i = "bathrooms")
# Use boosted model to predict price on the test set
boost_pred <- predict(boost_house,
newdata = best_test,
n.trees = 5000)
boost_MSE <- mean((boost_pred - testdata)^2)
boost_RMSE <- sqrt(boost_MSE)
# Create new boosted model but change lambda to 0.001
boost_model <- gbm(price ~ .,
data = best_train,
distribution = "gaussian",
n.trees = 5000,
interaction.depth = 4,
shrinkage = 0.001,
verbose = F)
boost_pred2 <- predict(boost_model,
newdata = best_test,
n.trees = 5000)
boost2_MSE <- mean((boost_pred2 - testdata)^2)
boost2_RMSE <- sqrt(boost2_MSE)
# Changing lambda has greatly improved accuracy, but is not competitive with
# other models.
# 12.0 Results ####
RMSE_comparison <- c(simple_lm_RMSE,
multiple_lm_RMSE,
multiple_select_RMSE,
final_bestsub_RMSE,
backward_RMSE,
forward_RMSE,
lasso_RMSE,
lasso2_RMSE,
lasso_RMSE2,
ridge_RMSE,
tree_RMSE,
rf_RMSE,
rf_100_RMSE,
rf_m3_RMSE,
rf_m3_30_RMSE,
boost_RMSE,
boost2_RMSE)
RMSE_comparison <- t(RMSE_comparison)
RMSE_names <- c("Simple Linear Model",
"Multiple Linear Model with all availanle predictors",
"Mutiple Linear Model with only 15 predictors",
"Linear Model, variables selected by best-subset selection",
"Linear Model, variables selected by backward stepwise selection",
"Linear Model, variables selected by forward stepwise selection",
"LASSO Regression Model",
"LASSO Regression Model with increased lambda",
"LASSO Regression Model only using observations with price under $1m",
"Ridge Regression Model",
"Basic Decision Tree",
"Bagging Model of randomForest, m = p",
"Bagging Model, reduced to 100 trees",
"randomForest, m = 3",
"randomForest, m = 30, reduced to 30 trees",
"Boosting with lambda = 0.1",
"Boosting with lambda = 0.001")
results_table <- cbind(RMSE_names, RMSE_comparison)
write.xlsx(results_table, "results.xlsx")
barplot(RMSE_comparison,
main = "Comparison of RMSE across models",
xlab = RMSE_names)
ggplot(data = RMSE_comparison)
# 13.0 Trialling Linear Regression without Location variables ####
new_train <- train[, -11]
new_test <- test[, -11]
linear_mod <- lm(price ~ ., data = new_train)
# Make predictions on test data set
linear_pred <- predict(linear_mod, new_test)
# calculate MSE
linear_MSE <- mean((new_test[, "price"] - linear_pred)^2)
# calculate RMSE
linear_RMSE <- sqrt(linear_MSE)
# print summary of model
summary(linear_mod)
plot(RMSE_comparison,
main = "Comparison of RMSE across models",
xlab = RMSE_names)
plot(RMSE_comparison,
main = "Comparison of RMSE across models",
xlab = RMSE_names)
results_table <- read.xlsx("results.xlsx")
results_table$V2 <- as.numeric(results_table$V2)
table3_headers <- c("Model", "RMSE")
kable(results_table, col.names = table3_headers)%>%
column_spec(1, border_left = T, width = "7cm") %>%
column_spec(2, border_right = T) #%>%
# row_spec(1, background = "orange") %>%
# row_spec(2, background = "green") %>%
# row_spec(3, background = "green") %>%
# row_spec(4, background = "coral") %>%
# row_spec(5, background = "coral") %>%
# row_spec(6, background = "coral") %>%
# row_spec(7, background = "lightgreen") %>%
# row_spec(8, background = "green") %>%
# row_spec(9, background = "darkorange") %>%
# row_spec(10, background = "darkorange") %>%
# row_spec(11, background = "darkorange") %>%
# row_spec(12, background = "orange") %>%
# row_spec(13, background = "orange") %>%
# row_spec(14, background = "red") %>%
# row_spec(15, background = "darkorange")
results_table <- read.xlsx("results.xlsx")
results_table$V2 <- as.numeric(results_table$V2)
table3_headers <- c("Model", "RMSE")
kable(results_table, col.names = table3_headers)%>%
column_spec(1, border_left = T, width = "7cm") %>%
column_spec(2, border_right = T) #%>%
results_table <- read.xlsx("results.xlsx")
RMSE_comparison <- t(RMSE_comparison)
RMSE_names <- c("Simple Linear Model",
"Multiple Linear Model with all availanle predictors",
"Mutiple Linear Model with only 15 predictors",
"Linear Model, variables selected by best-subset selection",
"Linear Model, variables selected by backward stepwise selection",
"Linear Model, variables selected by forward stepwise selection",
"LASSO Regression Model",
"LASSO Regression Model with increased lambda",
"LASSO Regression Model only using observations with price under $1m",
"Ridge Regression Model",
"Basic Decision Tree",
"Bagging Model of randomForest, m = p",
"Bagging Model, reduced to 100 trees",
"randomForest, m = 3",
"randomForest, m = 30, reduced to 30 trees",
"Boosting with lambda = 0.1",
"Boosting with lambda = 0.001")
results_table <- cbind(RMSE_names, RMSE_comparison)
write.xlsx(results_table, "results.xlsx")
results_table <- read.xlsx("results.xlsx")
results_table$V2 <- as.numeric(results_table$V2)
table3_headers <- c("Model", "RMSE")
kable(results_table, col.names = table3_headers)%>%
column_spec(1, border_left = T, width = "7cm") %>%
column_spec(2, border_right = T) #%>%
# Plot actual vs predicted prices for this LASSO model.
plot(test2[,"price"], lasso_pred2,
main = "Actual vs Predicted House Prices
from the LASSO Model",
xlab = "Test Set Prices (actual), US Dollars",
ylab = "Predicted Prices, US Dollars")
abline(a = 0, b = 1, col = "blue")
# Plot actual vs predicted prices for this LASSO model.
plot(test2[,"price"], lasso_pred2,
main = "Actual vs Predicted House Prices for LASSO Model
containing only house prices under $1million",
xlab = "Test Set Prices (actual), US Dollars",
ylab = "Predicted Prices, US Dollars")
# Plot actual vs predicted prices for this LASSO model.
plot(test2[,"price"], lasso_pred2,
main = "Actual vs Predicted House Prices for LASSO Model
containing only house prices under $1 million",
xlab = "Test Set Prices (actual), US Dollars",
ylab = "Predicted Prices, US Dollars")
abline(a = 0, b = 1, col = "blue")
knitr::include_graphics('/Users/florentinagalliers/Google Drive/Harper/1-C7081/Asssesment/github-C7081/C7081-assessment/actual-predicted-prices.png')
knitr::include_graphics("/Users/florentinagalliers/Google Drive/Harper/1-C7081/Asssesment/github-C7081/C7081-assessment/actual-vs-test-2.png")
# Only one variable (sqft_living) is used to construct a tree with 5 nodes
plot(tree_house)
text(tree_house,
pretty = 0,
cex = 0.8)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
# Plot the best tree rf_m3_30
fancyRpartPlot(rf_m3_30, main = "Regression Tree")
# Plot the best tree rf_m3_30
rpart_tree <- rpart(price ~ .,
data = best_train,
mtry = 3,
ntree = 30,
importance = TRUE)
# Plot the best tree rf_m3_30
rpart_tree <- rpart(price ~ .,
data = best_train,
importance = TRUE)
# Plot the best tree rf_m3_30
rpart_tree <- rpart(price ~ .,
data = best_train)
fancyRpartPlot(rpart_tree, main = "Regression Tree")
rpart.plot(rpart_tree, type = 5)
rpart.rules(rpart_tree)
prp(rpart_tree)
plot(rf_m3_30)
# Only one variable (sqft_living) is used to construct a tree with 5 nodes
plot(tree_house)
text(tree_house,
pretty = 0,
cex = 0.8)
# Only one variable (sqft_living) is used to construct a tree with 5 nodes
plot(tree_house,
main = "Simple Regression Tree")
text(tree_house,
pretty = 0,
cex = 0.8)
