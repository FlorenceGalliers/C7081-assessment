---
title: "Can we predict house prices using known features of each house and a supervised learning approach?"
author: "Florence Galliers"
date: "18/10/2020"
output: 
  pdf_document:
    toc: true #table of contents true, default depth 3
    number_sections: true
    df_print: [kable]
bibliography: ["ref.bib"]
biblio-style: "harvard"
link-citations: true
---

# Background:
House prices are an important part of the economy and usually reflect trends in it. They can be influenced by the physical condition of the house and by other attributes such as location [@bin2004prediction]. Prices are important for homeowners, prospective buyers and estate agents, prediction methods could help lead to more informed decisions to each of these stakeholders. @gao2019location suggest that prediction models may be useful for narrowing down the range of available houses for prospective buyers and allowing sellers to predict optimal times to list thier houses on the market. Prediction accuracy is important in all of these situations.

In most countries there is some form of house price index that measures changes in prices [@lu2017hybrid]. This contains a summary of all transactions that take place but not the individual features of each house sold, therefore it cannot be used to make predictions of house price.

Something else to take into account is that it may be difficult for prospective buyers to visualise how square footage measurements of a house are calculated or how this measurement translates into physical size if they have not visited the house themselves. Buyers therefore rely on factors such as the number of bedrooms, bathrooms or house age to get an idea of the value of the house. This analysis will focus on which features of a house have the largest influence on the prediction of house prices. This report does not look at the effect of time on house prices. It is already well known that house prices increase every year [@alfiyatin2017modeling]. 

Many house price prediction models have been created using machine learning methods. The hedonic price model is the most extensively researched and uses regression methods [@gao2019location]. The goal of regression methods is to build an equation which defines y, the dependent variable as a function of the x variable(s). This equation can then be used for prediction of y when given unseen values of x. Machine learning methods use data in a ‘training set’ to build a model, this model is then used to make predictions on an unseen ‘test set’ of data. The accuracy of models can be calculated by taking the predicted values from the actual values.

## Objectives:
* Understand which attributes of houses given in the data set can be used to effectively construct a prediction model for house price (dependent variable).
* Minimize the differences between predicted and actual house prices by using model selection to choose the most accurate model.

# Data:

## Data Description
The dataset chosen for this analysis is from houses sold in 2014 in Washington, USA. It contains the sale price (US dollars) along with attributes of each house such as number of bedrooms, number of bathrooms, etc. There were 4600 observations with 17 variables in the original data set downloaded from [Kaggle] (https://www.kaggle.com/shree1992/housedata).
Although the dataset is from 2014, it was a particularly interesting data set because it contains a large amount of information and an interesting selection of variables. A more recent data set, or one from the UK could not be found, and so this analysis will go ahead with this data set.

## Data Preparation and Exploration
The first task was preparing and cleaning the dataset. This served two purposes, firstly to get to know the different variables in the data and existing patterns or correlations between them and secondly to carry out feature selection. Missing values were searched for and removed and any observations in which price equalled zero were removed. The cleaned dataset was exported ready for use in the main analysis. This cleaned data set contained 4522 observations and 11 variables (Table 1).
```{r, Table 1, echo = FALSE}

```


## Data Visualisation
The cleaned data set was explored to look for any correlations between variables. The dependent variable (price) had a positive correlation with sqft_living, number of bathrooms and number of bedrooms. The strongest relationship seen was between sqft_living and number of bathrooms. It is interesting to note that the only non significant correlation involving price is between price and house age.

```{r, Correlation Plot, echo = FALSE}
 
```


# Analysis Methods
House price is a continuous variable and so a supervised learning approach was chosen throughout. In a supervised learning approach there is a response variable, in this case house price, and then a number of predictor variables. All the approaches tried were regression methods in which house price was a quantitative variable. 

The dataset was split randomly into two parts, a training set containing 80% of the observations and a test set containing the remaining 20%. The training set was used to train all of the models and the test set to assess accuracy of the models. 
 
## Linear Regression
To begin, a simple linear regression model was created using price as the dependent variable (y) and square foot living area (sqft_living) as the independent variable (x). Sqft_living was chosen because it was shown to be the most correlated variable to house price in the exploratory data analysis. 
A simple linear regression model uses the *lm()* function. Linear regression takes a model with equation

y = B0 + B1x + E

and estimates coefficients which produce a line of best fit, minimising the difference between predicted and actual values. This type of simple linear regression is known as ordinary least squares.

This simple model was then expanded to allow all the other variables in the dataset, this is multiple linear regression. The model output showed that 15 of these variables had a significant impact (P<0.01) on the price, they were:

* bedrooms
* bathrooms
* sqft_living
* if_basement1 
* house_age
* city.Bellevue
* city.Clydehill
* city.Issaquah
* city.Kent
* city.Kirkland
* city.Medina
* city.Mercerisland
* city.Redmond
* city.Sammamish
* city.Seattle 

It should be noted that *if_basement*, *if_renovated* and *city* were all factor variables, and so when they were fit into a model, they were converted into dummy variables, with one for each factor level. This is why some of the variables selected above are not the same as those variables shown in the cleaned data set. This also raises the idea that location has a large influence on house price as 10 of the most influential variables shown above are all dummy variables originating from *city*.

Multicollinearity was also explored from the multiple linear model. The variance inflation factors (VIF) for each variable was calculated which shows how much of the variance of the regression coefficient is inflated due to any multicollinearity in the model. None of the VIF were above 5, with the highest being *bathrooms* at 3.3, and so there was no problematic collinearity indicated. This was carried out using the *vif()* function in the *car* package.
 
## Variable Selection Methods
To look further into which variables were most influential on price, variable selection methods were explored. Best subset selection is a method that finds the best combinations of predictors that produce the best fit in terms of squared error. Forward selection is slightly different as it starts with no variables and one by one adds the variable which gives the smallest increase in squared error. Backward selection follows the same idea as forward selection but it starts with a full model, and iteratively removes variables until it leaves a one variable model with the lowest mean squared error. 

In best subset selection, cross-validation was used to choose among variables of differing sizes. This is a direct method of estimating test error. A model with 15 variables had the lowest cross-validation error. These variables were extracted and a linear model created containing only them. This linear model had a lower MSE than either of the simple or multiple linear models tried above. 

If we had used R2 statistics to assess the models, we would always end up with a model containing all of the variables as the 'best' one. In this analysis direct methods of test error estimation were used, however as an alternative, adjusted R2, CP or BIC criterion may have been used to indirectly estimate the test error. Validation set approach was also take to estimate test error, this gave a model with many more variables and so it was decided that cross-validation was the most appropriate method to use to help narrow down the variables to the most influential.

Forward and backward selection yielded almost identical results to each other with only a one variable difference to best-subset selection. The variables selected from best-subset selection were also very similar to the ones that showed a significant effect in the multiple linear model, with only one variable different. Forward and backward subset selection methods gave lower MSE results than best-subset selection.

Polynomial models of regression were trialed but they did not improve RMSE results, so this approach was not looked into further. The relationship between sqft_living and price looked to be linear in the exploratory data analysis so this was not a surprising development.

## Ridge Regression and LASSO
Ridge Regression and the LASSO are both shrinkage methods. In the above variable selection methods, only a subset of predictors are used. In shrinkage methods all of the predictors are included in the model but the coefficient estimates are constrained towards zero.
 
Ridge Regression utilises L2 regularisation, this adds a penalty to the coefficients that is equal to the square of the coefficients. Lambda is a tuning parameter, as its magnitude increases, the shrinkage penalty has more of an impact and the coefficient estimates will be closer to zero. Selecting the right value of lambda is very important, in this analysis it was selected using cross-validation. If lambda = 0, this method would be identical to ordinary least squares.

LASSO is another coefficient shrinkage technique in which the L0 norm is replaced with the L1 norm. This applies a penalty to the coefficients equal to the absolute value of the coefficients. In the LASSO method, lambda allows some coefficients to be set equal to 0, in which case they are dropped out of the regression model. In this way it  acts as a selection method for choosing the variables with the most influence. This decreases the variance of the model but increases the bias. We can change lambda to any value, but in this analysis cross-validation was used to select the best value of lambda. This method suggest a model containing 45 variables led to the lowest RMSE, this is all of the variables, so we might as well use multiple linear regression.
 
## Tree Based Methods
A few different tree based methods were explored in this analysis, starting with one simple decision tree, moving through bagging, randomForests and boosting. Tree based methods have the benefit of being easy to interpret but can sometimes over-simplify things.

Tree methods have a limit on the number of variables, so the reduced data set from forward selection was used throughout the tree methods.
 
Using just one decision tree gave a MSE higher than using just ordinary least squares, the pruned tree with the lowest cross validation error had the same number of branches as the original decision tree.

### Random Forests and Bagging
RandomForest is a method that combines together multiple decision trees, this can help to improve prediction accuracy. Bagging is a type of randomForest, also known as Bootsrap Aggregation, in which the number of predictors that is considered at each split of the tree is equal to the total number of predictors in the data set.

Random Forests only considers a subset of the predictors, usually sqrt(p) which in this case was around 3, at each split. This reduces variance and can lead to improved models. 

### Boosting

Boosting is another tree based method in which trees are grown sequentially, with each tree 'learning' from the last. It learns more slowly than other approaches and can reduce overfitting. Two different variations of a boosted model were created, with different values of lambda, the tuning parameter, and although reducing lambda improved MSE, it was not competitive with the multiple linear regression.

# Results:
Link to [Github](https://github.com/FlorenceGalliers/C7081-assessment) repository containing analysis script.

For regression problems the most common way to measure accuracy of a model is by minimising test error. The model that gave the lowest RMSE and therefore was the final approach chosen in this analysis is a linear model with predictor variables selected using forward selection. 15 variables were selected for this model and these were:

* bedrooms
* bathrooms
* sqft_living
* condition
* if_basement1
* house_age
* city.Bellevue
* city.Clydehill
* city.Issaquah
* city.Kirkland
* city.Medina
* city.Mercerisland
* city.Redmond
* city.Sammamish
* city.Seattle

It is clear that location has a large effect on house price as 9 of the variables selected in this model are all dummy variables from the original *city* variable. These selected variables only contain two differences (+condition -city.Kent) to the variables that had a significant impact on the multiple linear regression model that was fit with all of the available variables. 

The objectives of this analysis were to understand which attributes of the houses can be used to most effectively construct a prediction model for house price, and to then minimise the differences between predicted and actual house price using model selection.


## Conclusions:



# References
